{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d5a342",
   "metadata": {},
   "source": [
    "## Multibox_pior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a3cd8fec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multibox_prior(data, sizes, ratios):\r\n",
    "    \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\r\n",
    "    in_height, in_width = data.shape[1:3]\r\n",
    "    # print(in_height, in_width)\r\n",
    "    num_sizes, num_ratios = len(sizes), len(ratios)\r\n",
    "    boxes_per_pixel = (num_sizes + num_ratios - 1)\r\n",
    "    size_tensor = np.array(sizes, dtype=np.float32)\r\n",
    "    ratio_tensor = np.array(ratios, dtype=np.float32)\r\n",
    "    # Offsets are required to move the anchor to the center of a pixel. Since\r\n",
    "    # a pixel has height=1 and width=1, we choose to offset our centers by 0.5\r\n",
    "    offset_h, offset_w = 0.5, 0.5\r\n",
    "    steps_h = 1.0 / in_height  # Scaled steps in y-axis\r\n",
    "    steps_w = 1.0 / in_width  # Scaled steps in x-axis\r\n",
    "#     print(steps_h.dtype)\r\n",
    "\r\n",
    "    # Generate all center points for the anchor boxes\r\n",
    "    center_h = (np.arange(in_height, dtype=np.float32) + offset_h) * steps_h\r\n",
    "    center_w = (np.arange(in_width, dtype=np.float32) + offset_w) * steps_w\r\n",
    "    shift_x, shift_y = np.meshgrid(center_w, center_h)\r\n",
    "    shift_x, shift_y = shift_x.reshape(-1), shift_y.reshape(-1)\r\n",
    "\r\n",
    "\r\n",
    "    # Generate `boxes_per_pixel` number of heights and widths that are later\r\n",
    "    # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\r\n",
    "    w = np.concatenate((size_tensor * np.sqrt(ratio_tensor[0], dtype=np.float32),\r\n",
    "                        sizes[0] * np.sqrt(ratio_tensor[1:], dtype=np.float32))) \\\r\n",
    "                        * in_height / in_width  # Handle rectangular inputs\r\n",
    "    h = np.concatenate((size_tensor / np.sqrt(ratio_tensor[0], dtype=np.float32),\r\n",
    "                        sizes[0] / np.sqrt(ratio_tensor[1:], dtype=np.float32)))\r\n",
    "    # Divide by 2 to get half height and half width\r\n",
    "    anchor_manipulations = np.tile(\r\n",
    "        np.stack((-w, -h, w, h)).T, (in_height * in_width, 1)) / 2\r\n",
    "\r\n",
    "    # Each center point will have `boxes_per_pixel` number of anchor boxes, so\r\n",
    "    # generate a grid of all anchor box centers with `boxes_per_pixel` repeats\r\n",
    "    out_grid = np.stack([shift_x, shift_y, shift_x, shift_y],\r\n",
    "                        axis=1).repeat(boxes_per_pixel, axis=0)\r\n",
    "    output = out_grid + anchor_manipulations\r\n",
    "    return np.expand_dims(output, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54619a",
   "metadata": {},
   "source": [
    "## Bbox_to_rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bfd6907c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bbox_to_rect(bbox, color):\r\n",
    "    \"\"\"Convert bounding box to matplotlib format.\"\"\"\r\n",
    "    # Convert the bounding box (upper-left x, upper-left y, lower-right x,\r\n",
    "    # lower-right y) format to the matplotlib format: ((upper-left x,\r\n",
    "    # upper-left y), width, height)\r\n",
    "    return plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2] - bbox[0],\r\n",
    "                             height=bbox[3] - bbox[1], fill=False,\r\n",
    "                             edgecolor=color, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e87ebb",
   "metadata": {},
   "source": [
    "## Box_corner_to_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a2c8727d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def box_corner_to_center(boxes):\r\n",
    "    \"\"\"Convert from (upper-left, lower-right) to (center, width, height).\"\"\"\r\n",
    "    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\r\n",
    "    cx = (x1 + x2) / 2\r\n",
    "    cy = (y1 + y2) / 2\r\n",
    "    w = x2 - x1\r\n",
    "    h = y2 - y1\r\n",
    "    boxes = np.stack((cx, cy, w, h), axis=-1)\r\n",
    "    return boxes\r\n",
    "\r\n",
    "#@save\r\n",
    "def box_center_to_corner(boxes):\r\n",
    "    \"\"\"Convert from (center, width, height) to (upper-left, lower-right).\"\"\"\r\n",
    "    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\r\n",
    "    x1 = cx - 0.5 * w\r\n",
    "    y1 = cy - 0.5 * h\r\n",
    "    x2 = cx + 0.5 * w\r\n",
    "    y2 = cy + 0.5 * h\r\n",
    "    boxes = np.stack((x1, y1, x2, y2), axis=-1)\r\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01fd08",
   "metadata": {},
   "source": [
    "## box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a1100f76",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def box_iou(boxes1, boxes2):\r\n",
    "    \"\"\"Compute pairwise IoU across two lists of anchor or bounding boxes.\"\"\"\r\n",
    "    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\r\n",
    "                              (boxes[:, 3] - boxes[:, 1]))\r\n",
    "    # Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),\r\n",
    "    # (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)\r\n",
    "    areas1 = box_area(boxes1)\r\n",
    "    areas2 = box_area(boxes2)\r\n",
    "    # Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of\r\n",
    "    # boxes1, no. of boxes2, 2)\r\n",
    "    inter_upperlefts = np.maximum(boxes1[:, None, :2], boxes2[:, :2])\r\n",
    "    inter_lowerrights = np.minimum(boxes1[:, None, 2:], boxes2[:, 2:])\r\n",
    "    inters = (inter_lowerrights - inter_upperlefts).clip(min=0)\r\n",
    "    # Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)\r\n",
    "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\r\n",
    "    union_areas = areas1[:, None] + areas2 - inter_areas\r\n",
    "    return inter_areas / union_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db62b47c",
   "metadata": {},
   "source": [
    "## Assign_anchor_to_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "367e5ad9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_anchor_to_bbox(ground_truth, anchors, device=None, iou_threshold=0.5):\r\n",
    "    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\r\n",
    "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\r\n",
    "    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\r\n",
    "    # box i and the ground-truth bounding box j\r\n",
    "    jaccard = box_iou(anchors, ground_truth)\r\n",
    "    # Initialize the tensor to hold the assigned ground-truth bounding box for\r\n",
    "    # each anchor\r\n",
    "    anchors_bbox_map = np.full((num_anchors,), -1, dtype=np.int32)\r\n",
    "    # Assign ground-truth bounding boxes according to the threshold\r\n",
    "    max_ious, indices = np.max(jaccard, axis=1), np.argmax(jaccard, axis=1)\r\n",
    "    anc_i = np.nonzero(max_ious >= 0.5)[0]\r\n",
    "    box_j = indices[max_ious >= 0.5]\r\n",
    "    anchors_bbox_map[anc_i] = box_j\r\n",
    "    col_discard = np.full((num_anchors,), -1)\r\n",
    "    row_discard = np.full((num_gt_boxes,), -1)\r\n",
    "    for _ in range(num_gt_boxes):\r\n",
    "        max_idx = np.argmax(jaccard)  # Find the largest IoU\r\n",
    "        box_idx = (max_idx % num_gt_boxes).astype('int32')\r\n",
    "        anc_idx = (max_idx / num_gt_boxes).astype('int32')\r\n",
    "        anchors_bbox_map[anc_idx] = box_idx\r\n",
    "        jaccard[:, box_idx] = col_discard\r\n",
    "        jaccard[anc_idx, :] = row_discard\r\n",
    "    return anchors_bbox_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a1414",
   "metadata": {},
   "source": [
    "## Offset_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a32683eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def offset_boxes(anchors, assigned_bb, eps=1e-6):\r\n",
    "    \"\"\"Transform for anchor box offsets.\"\"\"\r\n",
    "    c_anc = box_corner_to_center(anchors)\r\n",
    "    c_assigned_bb = box_corner_to_center(assigned_bb)\r\n",
    "    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]\r\n",
    "    offset_wh = 5 * np.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])\r\n",
    "    offset = np.concatenate([offset_xy, offset_wh], axis=1)\r\n",
    "    return offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed487dc",
   "metadata": {},
   "source": [
    "## multibox_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c869324",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multibox_target(anchors, labels):\r\n",
    "    \"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\r\n",
    "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\r\n",
    "    batch_offset, batch_mask, batch_class_labels = [], [], []\r\n",
    "    num_anchors = anchors.shape[0]\r\n",
    "    for i in range(batch_size):\r\n",
    "        label = labels[i, :, :]\r\n",
    "        anchors_bbox_map = assign_anchor_to_bbox(label[:, 1:], anchors)\r\n",
    "                                                 \r\n",
    "        bbox_mask = np.tile((np.expand_dims(\r\n",
    "            (anchors_bbox_map >= 0), axis=-1)), (1, 4)).astype('float32')\r\n",
    "        # Initialize class labels and assigned bounding box coordinates with\r\n",
    "        # zeros\r\n",
    "        class_labels = np.zeros(num_anchors, dtype=np.int32)\r\n",
    "        assigned_bb = np.zeros((num_anchors, 4), dtype=np.float32,)\r\n",
    "        # Label classes of anchor boxes using their assigned ground-truth\r\n",
    "        # bounding boxes. If an anchor box is not assigned any, we label its\r\n",
    "        # class as background (the value remains zero)\r\n",
    "        indices_true = np.nonzero(anchors_bbox_map >= 0)[0]\r\n",
    "        bb_idx = anchors_bbox_map[indices_true]\r\n",
    "        class_labels[indices_true] = label[bb_idx, 0].astype('float32') + 1\r\n",
    "        assigned_bb[indices_true] = label[bb_idx, 1:]\r\n",
    "        # Offset transformation\r\n",
    "        \r\n",
    "        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\r\n",
    "        \r\n",
    "        batch_offset.append(offset.reshape(-1))\r\n",
    "        batch_mask.append(bbox_mask.reshape(-1))\r\n",
    "        batch_class_labels.append(class_labels.astype('float32'))\r\n",
    "    \r\n",
    "    bbox_offset = np.stack(batch_offset)\r\n",
    "    bbox_mask = np.stack(batch_mask)\r\n",
    "    # print(class_labels.dtype)\r\n",
    "    class_labels = np.stack(batch_class_labels)\r\n",
    "    \r\n",
    "    return (bbox_offset, bbox_mask, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11613ae2",
   "metadata": {},
   "source": [
    "## Offset_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7346e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def offset_inverse(anchors, offset_preds):\r\n",
    "    anc = box_corner_to_center(anchors)\r\n",
    "    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]\r\n",
    "    pred_bbox_wh = np.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]\r\n",
    "    pred_bbox = np.concatenate((pred_bbox_xy, pred_bbox_wh), axis=1)\r\n",
    "    predicted_bbox = box_center_to_corner(pred_bbox)\r\n",
    "    return predicted_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfea557",
   "metadata": {},
   "source": [
    "## nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9baa5759",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nms(boxes, scores, iou_threshold):\r\n",
    "    \"\"\"Sort confidence scores of predicted bounding boxes.\"\"\"\r\n",
    "    B = scores.argsort()[::-1]\r\n",
    "    keep = []  # Indices of predicted bounding boxes that will be kept\r\n",
    "    while B.size > 0:\r\n",
    "        i = B[0]\r\n",
    "        keep.append(i)\r\n",
    "        if B.size == 1: break\r\n",
    "        iou = box_iou(boxes[i, :].reshape(-1, 4),\r\n",
    "                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)\r\n",
    "        inds = np.nonzero(iou <= iou_threshold)[0]\r\n",
    "        B = B[inds + 1]\r\n",
    "    return np.array(keep, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1fee03",
   "metadata": {},
   "source": [
    "## multibox_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ff74d78f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,\r\n",
    "                       pos_threshold=0.009999999):\r\n",
    "    \"\"\"Predict bounding boxes using non-maximum suppression.\"\"\"\r\n",
    "    batch_size = cls_probs.shape[0]\r\n",
    "    anchors = np.squeeze(anchors, axis=0)\r\n",
    "    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]\r\n",
    "    out = []\r\n",
    "    for i in range(batch_size):\r\n",
    "        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)\r\n",
    "        conf, class_id = np.max(cls_prob[1:], 0), np.argmax(cls_prob[1:], 0)\r\n",
    "        predicted_bb = offset_inverse(anchors, offset_pred)\r\n",
    "        keep = nms(predicted_bb, conf, nms_threshold)\r\n",
    "        # Find all non-`keep` indices and set the class to background\r\n",
    "        all_idx = np.arange(num_anchors, dtype=np.int32)\r\n",
    "        combined = np.concatenate((keep, all_idx))\r\n",
    "        unique, counts = np.unique(combined, return_counts=True)\r\n",
    "        non_keep = unique[counts == 1]\r\n",
    "        all_id_sorted = np.concatenate((keep, non_keep))\r\n",
    "        class_id[non_keep] = -1\r\n",
    "        class_id = class_id[all_id_sorted].astype('float32')\r\n",
    "        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]\r\n",
    "        # Here `pos_threshold` is a threshold for positive (non-background)\r\n",
    "        # predictions\r\n",
    "        below_min_idx = (conf < pos_threshold)\r\n",
    "        class_id[below_min_idx] = -1\r\n",
    "        conf[below_min_idx] = 1 - conf[below_min_idx]\r\n",
    "        pred_info = np.concatenate((np.expand_dims(\r\n",
    "            class_id, axis=1), np.expand_dims(conf, axis=1), predicted_bb),\r\n",
    "                                   axis=1)\r\n",
    "        out.append(pred_info)\r\n",
    "    return np.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4bba9f5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib\r\n",
    "import os\r\n",
    "import tarfile\r\n",
    "import zipfile\r\n",
    "import requests\r\n",
    "\r\n",
    "#@save\r\n",
    "DATA_HUB = dict()\r\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a3894f0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_extract(name, folder=None):  #@save\r\n",
    "    \"\"\"Download and extract a zip/tar file.\"\"\"\r\n",
    "    fname = download(name)\r\n",
    "    base_dir = os.path.dirname(fname)\r\n",
    "    data_dir, ext = os.path.splitext(fname)\r\n",
    "    if ext == '.zip':\r\n",
    "        fp = zipfile.ZipFile(fname, 'r')\r\n",
    "    elif ext in ('.tar', '.gz'):\r\n",
    "        fp = tarfile.open(fname, 'r')\r\n",
    "    else:\r\n",
    "        assert False, 'Only zip/tar files can be extracted.'\r\n",
    "    fp.extractall(base_dir)\r\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir\r\n",
    "\r\n",
    "def download_all():  #@save\r\n",
    "    \"\"\"Download all files in the DATA_HUB.\"\"\"\r\n",
    "    for name in DATA_HUB:\r\n",
    "        download(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1df01639",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download(name, cache_dir=os.path.join('..', 'data')):  #@save\r\n",
    "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\r\n",
    "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\r\n",
    "    url, sha1_hash = DATA_HUB[name]\r\n",
    "    os.makedirs(cache_dir, exist_ok=True)\r\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\r\n",
    "    if os.path.exists(fname):\r\n",
    "        sha1 = hashlib.sha1()\r\n",
    "        with open(fname, 'rb') as f:\r\n",
    "            while True:\r\n",
    "                data = f.read(1048576)\r\n",
    "                if not data:\r\n",
    "                    break\r\n",
    "                sha1.update(data)\r\n",
    "        if sha1.hexdigest() == sha1_hash:\r\n",
    "            return fname  # Hit cache\r\n",
    "    print(f'Downloading {fname} from {url}...')\r\n",
    "    r = requests.get(url, stream=True, verify=True)\r\n",
    "    with open(fname, 'wb') as f:\r\n",
    "        f.write(r.content)\r\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1799e364",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "\r\n",
    "#@save\r\n",
    "DATA_HUB['banana-detection'] = (\r\n",
    "    DATA_URL + 'banana-detection.zip',\r\n",
    "    '5de26c8fce5ccdea9f91267273464dc968d20d72')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ef43330f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@save\r\n",
    "def read_data_bananas(is_train=True):\r\n",
    "    \"\"\"Read the banana detection dataset images and labels.\"\"\"\r\n",
    "    data_dir = download_extract('banana-detection')\r\n",
    "    csv_fname = os.path.join(data_dir,\r\n",
    "                             'bananas_train' if is_train else 'bananas_val',\r\n",
    "                             'label.csv')\r\n",
    "    csv_data = pd.read_csv(csv_fname)\r\n",
    "    csv_data = csv_data.set_index('img_name')\r\n",
    "    images, targets = [], []\r\n",
    "    for img_name, target in csv_data.iterrows():\r\n",
    "        images.append(\r\n",
    "            plt.imread(\r\n",
    "                os.path.join(data_dir,\r\n",
    "                             'bananas_train' if is_train else 'bananas_val',\r\n",
    "                             'images', f'{img_name}')))\r\n",
    "        # Here `target` contains (class, upper-left x, upper-left y,\r\n",
    "        # lower-right x, lower-right y), where all the images have the same\r\n",
    "        # banana class (index 0)\r\n",
    "        targets.append(list(target))\r\n",
    "    return images, np.expand_dims(np.array(targets), 1) / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6666cdde",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\r\n",
    "import cv2\r\n",
    "\r\n",
    "class DataGenerator(Sequence):\r\n",
    "    'Generates data for Keras'\r\n",
    "    def __init__(self,\r\n",
    "                 batch_size, is_train,\r\n",
    "                 shuffle=True):\r\n",
    "        '''\r\n",
    "        all_filenames: list toàn bộ các filename\r\n",
    "        labels: nhãn của toàn bộ các file\r\n",
    "        batch_size: kích thước của 1 batch\r\n",
    "        index2class: index của các class\r\n",
    "        input_dim: (width, height) đầu vào của ảnh\r\n",
    "        n_channels: số lượng channels của ảnh\r\n",
    "        n_classes: số lượng các class \r\n",
    "        shuffle: có shuffle dữ liệu sau mỗi epoch hay không?\r\n",
    "        '''\r\n",
    "        self.features, self.labels = read_data_bananas(is_train)\r\n",
    "        print('read ' + str(len(self.features)) + (\r\n",
    "            f' training examples' if is_train else f' validation examples'))\r\n",
    "        self.batch_size = batch_size\r\n",
    "        \r\n",
    "        self.shuffle = shuffle\r\n",
    "        self.on_epoch_end()\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        '''\r\n",
    "        return:\r\n",
    "          Trả về số lượng batch/1 epoch\r\n",
    "        '''\r\n",
    "        return int(np.floor(len(self.features) / self.batch_size))\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        '''\r\n",
    "        params:\r\n",
    "          index: index của batch\r\n",
    "        return:\r\n",
    "          X, y cho batch thứ index\r\n",
    "        '''\r\n",
    "        # Lấy ra indexes của batch thứ index\r\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n",
    "\r\n",
    "        # List all_filenames trong một batch\r\n",
    "        all_features = [self.features[k] for k in indexes]\r\n",
    "        all_labels = [self.labels[k] for k in indexes]\r\n",
    "        \r\n",
    "        all_features = np.asarray(all_features).astype(np.float32)\r\n",
    "        all_labels = np.asarray(all_labels).astype(np.float32)\r\n",
    "        \r\n",
    "        \r\n",
    "        \r\n",
    "\r\n",
    "        # Khởi tạo data\r\n",
    "#         X, y = self.__data_generation(all_features, all_labels)\r\n",
    "\r\n",
    "        return all_features, all_labels\r\n",
    "\r\n",
    "    def on_epoch_end(self):\r\n",
    "        '''\r\n",
    "        Shuffle dữ liệu khi epochs end hoặc start.\r\n",
    "        '''\r\n",
    "        self.indexes = np.arange(len(self.features))\r\n",
    "        if self.shuffle == True:\r\n",
    "            np.random.shuffle(self.indexes)\r\n",
    "\r\n",
    "    def __data_generation(self, all_features, all_labels):\r\n",
    "        '''\r\n",
    "        params:\r\n",
    "          all_filenames_temp: list các filenames trong 1 batch\r\n",
    "        return:\r\n",
    "          Trả về giá trị cho một batch.\r\n",
    "        '''\r\n",
    "        X = np.empty((self.batch_size, 256,256, 3), dtype=np.float32)\r\n",
    "        y = np.empty((self.batch_size, 1, 5), dtype=np.float32)\r\n",
    "\r\n",
    "        # Khởi tạo dữ liệu\r\n",
    "        for i, fn in enumerate(all_features, all_labels):\r\n",
    "            # Đọc file từ folder name\r\n",
    "            img = all_features.astype('float32')\r\n",
    "#             label = \r\n",
    "\r\n",
    "            label = fn.split('/')[3]\r\n",
    "            label = self.index2class[label]\r\n",
    "    \r\n",
    "            X[i,] = img\r\n",
    "\r\n",
    "            # Lưu class\r\n",
    "            y[i] = label\r\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ea99e5d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_bananas(batch_size):\r\n",
    "    \"\"\"Load the banana detection dataset.\"\"\"\r\n",
    "    train_iter = DataGenerator(batch_size, is_train=True, shuffle=True)\r\n",
    "    val_iter =DataGenerator(batch_size, is_train=False)\r\n",
    "    return train_iter, val_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d10ff9ab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 1000 training examples\n",
      "read 100 validation examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2, 256, 256, 3), (2, 1, 5))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, edge_size = 2, 256\r\n",
    "train_iter, _ = load_data_bananas(batch_size)\r\n",
    "batch = next(iter(train_iter))\r\n",
    "batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "19b41916",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 256, 256, 3), (2, 1, 5))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7122541-6115-4d4b-b0e0-30a5ac0f6a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99fd2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_predictor(num_anchors, num_classes):\r\n",
    "    return layers.Conv2D(num_anchors * (num_classes + 1), kernel_size=3,\r\n",
    "                     padding=\"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4deeb48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_predictor(num_anchors):\r\n",
    "    return layers.Conv2D(num_anchors * 4, kernel_size=3, padding=\"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f94c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "def flatten_pred(pred):\r\n",
    "    # print(tf.reshape(pred, shape=(-1,)))\r\n",
    "    return tf.reshape(pred, shape=(pred.shape[0], -1))\r\n",
    "\r\n",
    "def concat_preds(preds):\r\n",
    "    return tf.concat([flatten_pred(p) for p in preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c9c5957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample_blk(num_channels):\r\n",
    "    blk = tf.keras.Sequential()\r\n",
    "    for _ in range(2):\r\n",
    "        blk.add(layers.Conv2D(num_channels, kernel_size=3, padding=\"same\"))\r\n",
    "        blk.add(layers.BatchNormalization())\r\n",
    "        blk.add(layers.Activation('relu'))\r\n",
    "\r\n",
    "    blk.add(layers.MaxPool2D(2))\r\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "481b003e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11872/2328591092.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'forward' is not defined"
     ]
    }
   ],
   "source": [
    "def base_net():\r\n",
    "    blk = tf.keras.Sequential()\r\n",
    "    for num_filters in [16, 32, 64]:\r\n",
    "        blk.add(down_sample_blk(num_filters))\r\n",
    "    return blk\r\n",
    "\r\n",
    "forward(np.zeros((2, 256, 256, 3)), base_net()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5b762f89",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_blk(i):\r\n",
    "    if i == 0:\r\n",
    "        blk = base_net()\r\n",
    "    elif i == 4:\r\n",
    "        blk = layers.GlobalMaxPool2D(keepdims=True)\r\n",
    "    else:\r\n",
    "        blk = down_sample_blk(128)\r\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b645a474",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\r\n",
    "    Y = blk(X)\r\n",
    "    # print(Y.shape, size, ratio)\r\n",
    "    anchors = multibox_prior(Y, sizes=size, ratios=ratio)\r\n",
    "    # print(\"anchor\", anchors.shape)\r\n",
    "    cls_preds = cls_predictor(Y)\r\n",
    "    bbox_preds = bbox_predictor(Y)\r\n",
    "    return (Y, anchors, cls_preds, bbox_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca98ea-1ad6-43dd-8133-5ced44e578b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c287b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],\r\n",
    "         [0.88, 0.961]]\r\n",
    "ratios = [[1, 2, 0.5]] * 5\r\n",
    "num_anchors = len(sizes[0]) + len(ratios[0]) \n",
    "\n",
    "input_tf = np.random.rand(1,300,300,3).astype(\"float32\")- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3196f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class TinySSD(tf.keras.Model):\r\n",
    "    def __init__(self, num_classes, **kwargs):\r\n",
    "        super(TinySSD, self).__init__(**kwargs)\r\n",
    "        self.num_classes = num_classes\r\n",
    "        # for i in range(5):\r\n",
    "        #     # Equivalent to the assignment statement `self.blk_i = get_blk(i)`\r\n",
    "        #     setattr(self, f'blk_{i}', get_blk(i))\r\n",
    "        #     setattr(self, f'cls_{i}', cls_predictor(num_anchors, num_classes))\r\n",
    "        #     setattr(self, f'bbox_{i}', bbox_predictor(num_anchors))\r\n",
    "        self.blk = get_blk(0)\r\n",
    "        self.cls = cls_predictor(4,1)\r\n",
    "        self.bbox = bbox_predictor(4)\r\n",
    "\r\n",
    "    def call(self, X):\r\n",
    "        \r\n",
    "        # X = self.blk(X)\r\n",
    "        \r\n",
    "        X, anchors, cls_preds, bbox_preds = blk_forward(X, self.blk,  sizes[0], ratios[0], self.cls, self.bbox)\r\n",
    "        \r\n",
    "        print( anchors.shape, cls_preds.shape, bbox_preds.shape)\r\n",
    "        \r\n",
    "        # print(X.shape)\r\n",
    "        # anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\r\n",
    "        # for i in range(5):\r\n",
    "        #     # Here `getattr(self, 'blk_%d' % i)` accesses `self.blk_i`\r\n",
    "        #     # print(X.shape)\r\n",
    "        #     X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\r\n",
    "        #         X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],\r\n",
    "        #         getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))\r\n",
    "        #     # print(X.shape)\r\n",
    "        #     # print(anchors[i].shape)\r\n",
    "        # # print(anchors.shape)\r\n",
    "        # anchors = np.concatenate(anchors, axis=1)\r\n",
    "        # cls_preds = concat_preds(cls_preds)\r\n",
    "        cls_preds = tf.reshape(cls_preds, (cls_preds.shape[0], -1,self.num_classes + 1))\r\n",
    "        print(bbox_preds.shape)\r\n",
    "        bbox_preds = concat_preds([bbox_preds])\r\n",
    "        return anchors, cls_preds, bbox_preds\r\n",
    "        print(X.shape)\r\n",
    "        # return X, _, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3db51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "class Block_tf(tf.keras.Model):\r\n",
    "    \"\"\"Create Block convlution for Encode. Use layers convlution\r\n",
    "\r\n",
    "    Args:\r\n",
    "        torch ([Module pytorch]): [create block conv with class Module of pytorch]\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, mid_channel, out_channels, batch_norm=False):\r\n",
    "        super().__init__()\r\n",
    "        \r\n",
    "        self.conv1Conv2Ders.Conv2d(mid_channel, kernel_size=3, padding='same')\r\n",
    "        self.convConv2Dyers.Conv2d(out_channels, kernel_size=3, padding='same')\r\n",
    "        \r\n",
    "        self.batch_norm = batch_norm\r\n",
    "        if batch_norm:\r\n",
    "            self.bn1 = layers.BatchNormalization()\r\n",
    "            self.bn2 = layers.BatchNormalization()\r\n",
    "            \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        if self.batch_norm:\r\n",
    "            x = self.bn1(x)\r\n",
    "        x = tf.nn.relu(x)\r\n",
    "        \r\n",
    " s       x = self.conv2(x)\r\n",
    "        if self.batch_norm:\r\n",
    "            x = self.bn2(x)\r\n",
    "        out = tf.nn.relu(x)\r\n",
    "        return out\r\n",
    "\r\n",
    "    @property\r\n",
    "    def _get_conv(self):\r\n",
    "        name_layer = [\"conv1\", \"conv2\"]\r\n",
    "        \r\n",
    "        # [getattr(self, i) for i in name_layer]\r\n",
    "        \r\n",
    "        return [getattr(self, i) for i in name_layer]\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eba61cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MySSD_tf(tf.keras.Model):\n",
    "    def down(self, x):\n",
    "        return layers.MaxPool2D(2)\n",
    "    def __init__(self, num_classes=None, batch_norm=False, upscale_mode=\"nearest\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.upscale_mode = upscale_mode\n",
    "\n",
    "        self.sizes = [[0.2, 0.272], [0.37, 0.447], \n",
    "                        [0.54, 0.619], [0.71, 0.79], \n",
    "                        [0.88, 0.961]]\n",
    "        self.ratios = [[1, 2, 0.5]] * 5\n",
    "        self.num_anchors = [len(size) + len(ratio) - 1 for size, ratio in zip(self.sizes, self.ratios)]\n",
    "\n",
    "        self.auxiliaryconvs = AuxiliaryConvolutions_tf()\n",
    "        \n",
    "        # encoder\n",
    "        self.enc1 = Block_tf(16, 16, batch_norm)\n",
    "        self.enc2 = Block_tf(32, 32, batch_norm)\n",
    "        self.enc3 = Block_tf(64, 64, batch_norm)\n",
    "        self.enc4 = Block_tf(128, 128, batch_norm)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        idx_to_in_channels = [128, 128, 128, 128, 128]\n",
    "        for i in range(len(idx_to_in_channels)):\n",
    "            setattr(self, f'cls_{i}', cls_predictor(self.num_anchors[i],\n",
    "                              self.num_classes))\n",
    "            setattr(self, f'bbox_{i}', bbox_predictor(self.num_anchors[i]))\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
    "\n",
    "        enc1 = self.enc1(x) # 300x300\n",
    "        enc2 = self.enc2(self.down(enc1)) # 150x150\n",
    "        enc3 = self.enc3(self.down(enc2)) # 75x75\n",
    "        enc4 = self.enc4(self.down(enc3)) # 37x37\n",
    "\n",
    "        enc5, enc6, enc7, enc8 = self.auxiliaryconvs(enc4)\n",
    "\n",
    "        for i, enc in enumerate([env4, enc5, enc6, enc7, enc8]):\n",
    "\n",
    "            anchor = multibox_prior(enc, self.sizes[i], self.ratios[i])\n",
    "\n",
    "            cls_pred = getattr(self, f'cls_{i}')(enc)\n",
    "\n",
    "            bbox_pred = getattr(self, f'bbox_{i}')(enc)\n",
    "            self.anchors[i] = anchor\n",
    "            cls_preds[i] = cls_pred\n",
    "            bbox_preds[i] = bbox_pred\n",
    "\n",
    "        self.anchors = tf.concat(self.anchors, dims=1)\n",
    "\n",
    "        cls_preds = concat_preds(cls_preds)\n",
    "\n",
    "        cls_preds = tf.reshape(cls_preds, (cls_preds.shape[0], -1,\n",
    "                                      self.num_classes + 1))\n",
    "        bbox_preds = concat_preds(bbox_preds)\n",
    "\n",
    "        assert cls_preds.shape[2] == self.num_classes + 1\n",
    "        assert self.anchors.shape[1] == cls_preds.shape[1] == int(bbox_preds.shape[1]/4)\n",
    "        return [self.anchors, cls_preds, bbox_preds]\n",
    "\n",
    "    @property\n",
    "    def _get_conv(self):\n",
    "        name_layer = [\"enc1\", \"enc2\", \"enc3\", \"enc4\"]\n",
    "        return [getattr(self, i) for i in name_layer]\n",
    "\n",
    "    @property\n",
    "    def _get_cls(self):\n",
    "        name_layer = [\"cls_1\", \"cls_2\", \"cls_3\", \"cls_4\"]\n",
    "        return [getattr(self, i) for i in name_layer]\n",
    "\n",
    "    @property\n",
    "    def _get_auxiliaryconv(self):\n",
    "        name_layer = [\"bbox_1\", \"bbox_2\", \"bbox_3\", \"bbox_4\"]\n",
    "        return [getattr(self, i) for i in name_layer]\n",
    "\n",
    "    @property\n",
    "    def _get_bbox(self):\n",
    "        name_layer = [\"auxiliaryconvs\"]\n",
    "        return [getattr(self, i) for i in name_layer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50f67f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AuxiliaryConvolutions_tf(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Additional convolutions to produce higher-level feature maps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions_tf, self).__init__()\n",
    "\n",
    "        # Auxiliary/additional convolutions on top of the VGG base\n",
    "        self.conv1_1 = layers.Conv2D(128, kernel_size=1)  # stride = 1, by default\n",
    "        self.conv1_2 = layers.Conv2D(128, kernel_size=1, strides=2)  # dim. reduction because stride > 1\n",
    "\n",
    "        self.conv2_1 = layers.Conv2D(128, kernel_size=1,)\n",
    "        self.conv2_2 = layers.Conv2D(128, kernel_size=3, strides=2)  # dim. reduction because stride > 1\n",
    "\n",
    "        self.conv3_1 = layers.Conv2D(128, kernel_size=1, )\n",
    "        self.conv3_2 = layers.Conv2D(128, kernel_size=3, strides=2)  # dim. reduction because padding = 0\n",
    "\n",
    "        self.conv4_1 = layers.Conv2D(128, kernel_size=1)\n",
    "        self.conv4_2 = layers.Conv2D(128, kernel_size=3)  # dim. reduction because padding = 0\n",
    "\n",
    "    def forward(self, conv4):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param conv7_feats: lower-level conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
    "        :return: higher-level feature maps conv8_2, conv9_2, conv10_2, and conv11_2\n",
    "        \"\"\"\n",
    "        # print(conv7_feats.shape)\n",
    "        out = tf.nn.relu(self.conv1_1(conv4))  # (N, 128, 37, 37)\n",
    "        out = tf.nn.relu(self.conv1_2(out))  # (N, 128, 18, 18)\n",
    "        # print(out.shape)\n",
    "        conv1_2_feats = out  # (N, 128, 10, 10)\n",
    "        \n",
    "\n",
    "        out = tf.nn.relu(self.conv2_1(out))  # (N, 128, 18, 18)\n",
    "        out = tf.nn.relu(self.conv2_2(out))  # (N, 128, 9, 9)\n",
    "        # print(out.shape)\n",
    "        conv2_2_feats = out  # (N, 128, 5, 5)\n",
    "\n",
    "        out = F.relu(self.conv3_1(out))  # (N, 128, 9, 9)\n",
    "        out = F.relu(self.conv3_2(out))  # (N, 128, 4, 4)\n",
    "        # print(out.shape)\n",
    "        conv3_2_feats = out  # (N, 128, 3, 3)\n",
    "\n",
    "        out = F.relu(self.conv4_1(out))  # (N, 128, 4, 4)\n",
    "        conv4_2_feats = F.relu(self.conv4_2(out))  # (N, 128, 2, 2)\n",
    "        # print(conv11_2_feats.shape)\n",
    "\n",
    "        # Higher-level feature maps\n",
    "        return conv1_2_feats, conv2_2_feats, conv3_2_feats, conv4_2_feats\n",
    "\n",
    "    @property\n",
    "    def _get_bbox(self):\n",
    "        name_layer = [\"auxiliaryconvs\"]\n",
    "        return [getattr(self, i) for i in name_layer]\n",
    "\n",
    "    @property\n",
    "    def _get_bbox(self):\n",
    "        name_layer = [\"conv1_1\", \"conv1_2\", \"conv2_1\", \"conv2_2\", \n",
    "                        \"conv3_1\", \"conv3_2\", \"conv4_1\", \"conv4_2\"]\n",
    "        return [getattr(self, i) for i in name_layer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdfb02bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.layers' has no attribute 'Conv2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11872/3743040973.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMySSD_tf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11872/743435201.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_classes, batch_norm, upscale_mode)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlock_tf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlock_tf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlock_tf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11872/2795064305.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mid_channel, out_channels, batch_norm)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmid_channel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers' has no attribute 'Conv2d'"
     ]
    }
   ],
   "source": [
    "model_tf = MySSD_tf(num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b88671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c3041b9cfe94553bf0fd6d1f91163b789f5886f7216f6b34a016e1a2f112d12"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
